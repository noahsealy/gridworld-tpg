{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 11 for testing on this\n",
    "class Figure11:\n",
    "    def __init__ (self, rows, cols, win_state, start_state, memory_size,\n",
    "                    legal_move, illegal_move, out_of_bounds, memory_repeat, goal_reached):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = memory_size #20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        self.legal_move = legal_move\n",
    "        self.illegal_move = illegal_move\n",
    "        self.out_of_bounds = out_of_bounds\n",
    "        self.memory_repeat = memory_repeat\n",
    "        self.goal_reached = goal_reached\n",
    "        \n",
    "    def sample_action (self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if (rand >= 0) and (rand < 0.25):\n",
    "            return 0\n",
    "        elif (rand >= 0.25) and (rand < 0.5):\n",
    "            return 1\n",
    "        elif (rand >= 0.5) and (rand < 0.75):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        \n",
    "    def reset (self):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (1, 2)) or (next == (1, 3)) or (next == (2, 2)) or (next == (2, 3)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += self.legal_move\n",
    "                #reward += 0.1\n",
    "                #reward -= 0.01\n",
    "            else:\n",
    "                reward += self.illegal_move\n",
    "                #reward -= 0.01\n",
    "                #reward -= 1\n",
    "                #reward = reward\n",
    "        else:\n",
    "            reward += self.out_of_bounds\n",
    "            #reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            reward += self.memory_repeat\n",
    "            #reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += self.goal_reached\n",
    "            #reward += 100\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate\n",
    "    \n",
    "    def animate_path(self, sequence):\n",
    "        current_map = np.zeros((5, 5))\n",
    "        # add barrier\n",
    "        current_map[(1, 2)] = 5\n",
    "        current_map[(1, 3)] = 5\n",
    "        current_map[(2, 2)] = 5\n",
    "        current_map[(2, 3)] = 5\n",
    "        current_map[self.win_state] = 8\n",
    "\n",
    "        # animate the run!\n",
    "        for i in range(len(sequence)):\n",
    "            time.sleep(0.5)\n",
    "            if i == 0:\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(0)\n",
    "                print(current_map)\n",
    "            else:\n",
    "                current_map[sequence[i-1]] = 0\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(i)\n",
    "                print(current_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 13 for testing on this\n",
    "class Figure13:\n",
    "    def __init__ (self, rows, cols, win_state, start_state, memory_size,\n",
    "                      legal_move, illegal_move, out_of_bounds, memory_repeat, goal_reached):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = memory_size #20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        self.legal_move = legal_move\n",
    "        self.illegal_move = illegal_move\n",
    "        self.out_of_bounds = out_of_bounds\n",
    "        self.memory_repeat = memory_repeat\n",
    "        self.goal_reached = goal_reached\n",
    "        \n",
    "    def reset (self):\n",
    "        self.current_state = self.start_state\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (2, 0)) or (next == (1, 1)) or (next == (2, 1)) or (next == (1, 3)) or (next == (2, 3)) or (next == (3, 3)) or (next == (3, 4)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += self.legal_move\n",
    "                #reward -= 0.01\n",
    "            else:\n",
    "                reward += self.illegal_move\n",
    "                #reward -= 1\n",
    "                #reward = reward\n",
    "        else:\n",
    "            reward += self.out_of_bounds\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            reward += self.memory_repeat\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += self.goal_reached\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (team, state):\n",
    "    top_learner = None\n",
    "    action = None   \n",
    "\n",
    "    # get best learner\n",
    "    actVars = {'frameNum':random.randrange(0, 100000000)}\n",
    "\n",
    "    #valid_learners = [lrnr for lrnr in team.learners if lrnr.isActionAtomic()]\n",
    "    valid_learners = [lrnr for lrnr in team.learners]\n",
    "    top_learner = max(valid_learners, key=lambda lrnr: lrnr.bid(state, actVars=actVars))\n",
    "    \n",
    "    if top_learner == None:\n",
    "        print('No top learner found!')\n",
    "        return None, 0\n",
    "    else:\n",
    "        actions = []\n",
    "#         top_q = 0\n",
    "        top_q = -10000000\n",
    "        top_action = None\n",
    "        \n",
    "        for entry in team.q_table:\n",
    "            if entry['learner'] == str(top_learner.id):\n",
    "                actions.append(entry['action'])\n",
    "                if entry['q'] >= top_q: # greater than OR greater than or equal to??\n",
    "                    top_q = entry['q']\n",
    "                    top_action = entry['action']            \n",
    "\n",
    "    # we don't use e-greedy here as exploration is not for testing, only training\n",
    "    return top_learner, top_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a given team after training\n",
    "def post_training_run (env_name, team):\n",
    "    env.reset()\n",
    "    l_t, a_t = evaluate(team, env.current_state)\n",
    "    actions = []\n",
    "    learners = []\n",
    "    states = []\n",
    "    #print(states)\n",
    "    states.append(env.current_state)\n",
    "    learners.append(l_t)\n",
    "    actions.append(a_t)\n",
    "    t = 0\n",
    "    if env_name == 'fig9':\n",
    "        t_max = 100\n",
    "    else:\n",
    "        t_max = 100\n",
    "    total_reward = 0\n",
    "    while t < t_max:\n",
    "        s_next, reward, isDone = env.step(a_t)\n",
    "        states.append(s_next)\n",
    "        learners.append(l_t)\n",
    "        actions.append(a_t)\n",
    "        total_reward += reward\n",
    "        if isDone:\n",
    "            return states, actions, learners, total_reward\n",
    "        l_next, a_next = evaluate(team, env.current_state)\n",
    "        \n",
    "#         if l_t.id != l_next.id:\n",
    "#             update(team, l_next, a_t, l_t, reward, alpha, epsilon)\n",
    "            \n",
    "        a_t = a_next\n",
    "        l_t = l_next\n",
    "        t = t + 1\n",
    "    return states, actions, learners, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to run a single agent on the environment, compatible with multiprocessing.\n",
    "\"\"\"\n",
    "def run_agent(args):\n",
    "    agent = args[0] # the agent\n",
    "    env_name = args[1] # name of environment\n",
    "    episodes = args[2] # number of times to repeat game\n",
    "    frames = args[3] # frames to play for\n",
    "\n",
    "    agent.configFunctionsSelf()\n",
    "    agent.zeroRegisters()\n",
    "\n",
    "    scores = []\n",
    "    states = []\n",
    "\n",
    "    for ep in range(episodes): # episode loop\n",
    "    \n",
    "        agent.zeroRegisters()\n",
    "        env = Figure11(5, 5, (0, 4), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "        state = env.reset()\n",
    "        score_ep = 0\n",
    "        for i in range(frames): # frame loop\n",
    "            act = agent.act(state)\n",
    "            \n",
    "            state, reward, is_done = env.step(act)\n",
    "            states.append(state)\n",
    "            score_ep += reward # accumulate reward in score\n",
    "            if is_done:\n",
    "                break # end early if losing state\n",
    "\n",
    "        scores.append(score_ep)\n",
    "        print(f\"Ep: {ep}, Score: {score_ep}\")\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    score = stat.mean(scores)\n",
    "    return score, states \n",
    "#     agent.reward(scores, \"Scores\")\n",
    "#     agent.reward(final_score, \"Mean\")\n",
    "    \n",
    "#     print(f\"Mean: {final_score}\")\n",
    "    \n",
    "#     agent.reward(final_score, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics as stat\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environmental parameters\n",
    "# envName = 'basegpfig11'\n",
    "# envName = 'basegpfig12'\n",
    "# envName = 'basegpfig13'\n",
    "# envName = 'randomstartsbasegpfig13'\n",
    "# envName = 'fig9'\n",
    "envName = 'fig11'\n",
    "# envName = 'fig12'\n",
    "# envName = 'fig13'\n",
    "# envName = 'oppositesamplesfig13'\n",
    "# envName = 'leftgoalfig12'\n",
    "# envName = 'leftgoalfig13'\n",
    "# envName = 'randomstartsfig9'\n",
    "# envName = 'randomstartsfig11'\n",
    "# envName = 'randomstartsfig12'\n",
    "# envName = 'randomstartsfig13'\n",
    "goalReached = 100\n",
    "memorySize = 20\n",
    "legalMove = 0.1\n",
    "illegalMove = -0.01\n",
    "outOfBounds = -0.01\n",
    "memoryRepeat = -0.01\n",
    "goalReached = 100\n",
    "# RL parameters\n",
    "epsilon = 0.1\n",
    "alpha = 0.05\n",
    "discount = 0.45#0.9\n",
    "# run parameters\n",
    "lamarckian = 0\n",
    "# trainer parameters\n",
    "# initMaxProgSize = 48\n",
    "\n",
    "initMaxProgSize =24#64\n",
    "nRegisters =4#8\n",
    "nActRegisters = 0\n",
    "initMaxActProgSize = 0\n",
    "\n",
    "pLrnDel=0.3#0.7\n",
    "pLrnAdd=0.2#0.7\n",
    "pLrnMut=0.7#0.3\n",
    "pProgMut=0.5#0.66\n",
    "pActMut=0.0#0.7#0.33\n",
    "pInstDel=0.5\n",
    "pInstAdd=0.5\n",
    "pInstSwp=0.5\n",
    "pInstMut=0.5\n",
    "\n",
    "initMaxTeamSize = 2\n",
    "maxTeamSize = 4#5#4\n",
    "gap = 0.5\n",
    "\n",
    "teamPopSize = 50\n",
    "pActAtom = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpg.agent import loadAgent\n",
    "def load_champion(agent_path):\n",
    "    agent = loadAgent(agent_path)\n",
    "    agent.zeroRegisters()\n",
    "    agent.configFunctionsSelf()\n",
    "    return agent\n",
    "    #return agent.team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved-runs/fig11_20_0.1_-0.01_-0.01_-0.01_100_50_1.0_4_24_0_0_2_4_0.5_0.3_0.2_0.7_0.5_0.0_0.5_0.5_0.5_0.5_0_0.1_0.05_0.45_19_80.pk1\n"
     ]
    }
   ],
   "source": [
    "# reconstruct the agent, using the parameters in the file name\n",
    "run = 19\n",
    "gen = 80\n",
    "file_name = 'saved-runs/'+envName+'_'+str(memorySize)+'_'+str(legalMove)+'_'+str(illegalMove)+'_'+str(outOfBounds)+'_'+str(memoryRepeat)+'_'+str(goalReached)+'_'+str(teamPopSize)+'_'+str(pActAtom)+'_'+str(nRegisters)+'_'+str(initMaxProgSize)+'_'+str(nActRegisters)+'_'+str(initMaxActProgSize)+'_'+str(initMaxTeamSize)+'_'+str(maxTeamSize)+'_'+str(gap)+'_'+str(pLrnDel)+'_'+str(pLrnAdd)+'_'+str(pLrnMut)+'_'+str(pProgMut)+'_'+str(pActMut)+'_'+str(pInstDel)+'_'+str(pInstAdd)+'_'+str(pInstSwp)+'_'+str(pInstMut)+'_'+str(lamarckian)+'_'+str(epsilon)+'_'+str(alpha)+'_'+str(discount)+'_'+str(run)+'_'+str(gen)+'.pk1'\n",
    "print(file_name)\n",
    "champ = load_champion(file_name)\n",
    "\n",
    "if (envName == 'fig11') or (envName == 'randomstartsfig11'):\n",
    "    env = Figure11(5, 5, (0, 4), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "elif (envName == 'fig12') or (envName == 'randomstartsfig12'):\n",
    "    env = Figure12(5, 5, (0, 4), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "elif envName == 'leftgoalfig12':\n",
    "    env = Figure12(5, 5, (0, 0), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "elif envName == 'leftgoalfig13':\n",
    "    env = Figure13(5, 5, (2, 0), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "else: # fig13 and randomstartsfig13\n",
    "    env = Figure13(5, 5, (2, 4), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731ff966-bc98-4dfa-b5f3-34c1e8a787a7 3 0.005909135951139496\n",
      "731ff966-bc98-4dfa-b5f3-34c1e8a787a7 0 0.09567360168840446\n",
      "9eb56e18-71dc-4e5a-9b74-7e2fd05d8134 3 0.02454999364004242\n",
      "9eb56e18-71dc-4e5a-9b74-7e2fd05d8134 2 0.0949268810857287\n",
      "Run: 0 -----\n",
      "100.74\n",
      "Run: 1 -----\n",
      "100.74\n",
      "Run: 2 -----\n",
      "100.74\n",
      "Run: 3 -----\n",
      "100.74\n",
      "Run: 4 -----\n",
      "100.74\n",
      "Run: 5 -----\n",
      "100.74\n",
      "Run: 6 -----\n",
      "100.74\n",
      "Run: 7 -----\n",
      "100.74\n",
      "Run: 8 -----\n",
      "100.74\n",
      "Run: 9 -----\n",
      "100.74\n",
      "731ff966-bc98-4dfa-b5f3-34c1e8a787a7 3 0.005909135951139496\n",
      "731ff966-bc98-4dfa-b5f3-34c1e8a787a7 0 0.09567360168840446\n",
      "9eb56e18-71dc-4e5a-9b74-7e2fd05d8134 3 0.02454999364004242\n",
      "9eb56e18-71dc-4e5a-9b74-7e2fd05d8134 2 0.0949268810857287\n"
     ]
    }
   ],
   "source": [
    "if envName == 'basegpfig11':\n",
    "    score, states = run_agent((champ, envName, 10, 100))\n",
    "    print(score)\n",
    "else:\n",
    "    for q_value in champ.team.q_table:\n",
    "        print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "    \n",
    "    for i in range(10):\n",
    "        champ.zeroRegisters()\n",
    "#         champ.configFunctionsSelf()\n",
    "        champ_team = champ.team\n",
    "        print('Run: ' + str(i) + ' -----')\n",
    "        states, actions, learners, score = post_training_run(envName, champ_team)\n",
    "        print(score)\n",
    "#         for j in range(len(states)):\n",
    "#             print(str(states[j]) + '  Action: ' + str(actions[j]) + ' Learner: ' + str(learners[j].id))\n",
    "#         print('\\n\\n')\n",
    "\n",
    "    for q_value in champ.team.q_table:\n",
    "        print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
