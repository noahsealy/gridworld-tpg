{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qtpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downing fig 11 GridWorld "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridWorld is based on fig 11 from Downing's \"Reinforced Genetic Programming\" paper\n",
    "#### As shown below, it is a 5x5 maze\n",
    "![DowningFig11](downing_fig_11.png)\n",
    "#### The current implementation involves the following rewards per action:\n",
    "#####  +100 to score upon reaching the goal\n",
    "#####  -0.01 to score per action\n",
    "#####  -1 to score upon repeating a state that was repeated n amount of times (n currently is 20)\n",
    "#####  -1 to score upon hitting a wall or going out of bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 11 for testing on this\n",
    "class GridWorld:\n",
    "    def __init__ (self, rows, cols, win_state, start_state):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = 20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def sample_action (self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if (rand >= 0) and (rand < 0.25):\n",
    "            return 0\n",
    "        elif (rand >= 0.25) and (rand < 0.5):\n",
    "            return 1\n",
    "        elif (rand >= 0.5) and (rand < 0.75):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        \n",
    "    def reset (self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (1, 2)) or (next == (1, 3)) or (next == (2, 2)) or (next == (2, 3)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += 0.01\n",
    "            else:\n",
    "                #reward -= 1\n",
    "                reward += 0\n",
    "        else:\n",
    "            #reward -= 1\n",
    "            reward += 0\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            #reward -= 1\n",
    "            reward += 0\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += 100\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_table: \n",
    "    def __init__ (self):\n",
    "        self.q = []\n",
    "    \n",
    "    def create (self, agents):\n",
    "        for agent in agents:\n",
    "            team = agent.team\n",
    "            for learner in team.learners:\n",
    "                (self.q).append({'team': str(team.id), 'learner': str(learner.id), 'action': learner.actionObj.actionCode, 'q': 0})\n",
    "    \n",
    "    def update (self, team_id, learner_id, action, q_value):\n",
    "        (self.q).append({'team': str(team_id), 'learner': str(learner_id), 'action': action, 'q': q_value})\n",
    "    \n",
    "    def display (self):\n",
    "        for entry in self.q:\n",
    "            print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learners (team):\n",
    "    print('Getting learners for team: ' + str(team.id))\n",
    "    return team.learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (team, state, epsilon, q_table):\n",
    "    #learners = get_learners(team)\n",
    "    #top_bid = 0\n",
    "    top_learner = None\n",
    "    action = None   \n",
    "\n",
    "    # get best learner\n",
    "    actVars = {'frameNum':-1}\n",
    "    valid_learners = [lrnr for lrnr in team.learners if lrnr.isActionAtomic()]\n",
    "    top_learner = max(valid_learners, key=lambda lrnr: lrnr.bid(state, actVars=actVars))\n",
    "\n",
    "#     for learner in learners:\n",
    "#         bid = learner.bid(state)\n",
    "#         if (bid > top_bid):\n",
    "#             top_bid = bid\n",
    "#             top_learner = learner \n",
    "\n",
    "    if top_learner == None:\n",
    "        print('No top learner found!')\n",
    "        return None, 0\n",
    "    else:\n",
    "        # e greedy action selection\n",
    "        e_prob = random.uniform(0, 1)\n",
    "\n",
    "        actions = []\n",
    "        top_q = 0\n",
    "        top_action = None\n",
    "        for entry in q_table.q:\n",
    "#             print('Entry team id: ' + entry['team'])\n",
    "#             print('Team id: ' + team.id)\n",
    "#             print('Entry learner id: ' + entry['learner'])\n",
    "#             print('Top learner id: ' + top_learner.id)\n",
    "#             print(' ')\n",
    "#             print(' ')\n",
    "            if (entry['team'] == str(team.id)) and (entry['learner'] == str(top_learner.id)):\n",
    "                actions.append(entry['action'])\n",
    "                #print('Action: ' + str(entry['action']))\n",
    "                if entry['q'] > top_q:\n",
    "                    top_q = entry['q']\n",
    "                    top_action = entry['action']\n",
    "        \n",
    "        #print('Actions: ' + str(len(actions)))\n",
    "\n",
    "        if e_prob < epsilon:\n",
    "            if len(actions) == 1:\n",
    "                action = actions[0]\n",
    "            else:\n",
    "                rand_action = random.randint(0, len(actions)-1)\n",
    "                action = actions[rand_action]\n",
    "        else:\n",
    "            # select action with highest q value from top learner's actions\n",
    "            action = top_action\n",
    "    \n",
    "    return top_learner, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update (q_table, team, next_learner, action, learner, reward):\n",
    "    alpha = 0.1\n",
    "    discount = 0.1\n",
    "    \n",
    "    # find the greatest q value out of possible actions for learner t+1\n",
    "    second_max_q = 0\n",
    "    for second_learner in q_table.q:\n",
    "        if second_learner['team'] == team.id and second_learner['learner'] == next_learner.id:\n",
    "            if second_learner['q'] > second_max_q:\n",
    "                second_max_q = second_learner['q']\n",
    "    \n",
    "    # find the current learner and q update\n",
    "    for first_learner in q_table.q:\n",
    "        if first_learner['team'] == team.id and first_learner['learner'] == learner.id and first_learner['action'] == action:\n",
    "            # equation 1 from tpg pdf\n",
    "            first_learner['q'] += alpha * (reward + (discount * second_max_q) - first_learner['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fitness (q_table, team, env):\n",
    "    epsilon = 0.2 # where should I define this??\n",
    "    l_t, a_t = evaluate(team, env.current_state, epsilon, q_table)\n",
    "    t = 0\n",
    "    t_max = 10000\n",
    "    total_reward = 0\n",
    "    while t < t_max:\n",
    "        s_next, reward, isDone = env.step(a_t)\n",
    "        #print(reward)\n",
    "        total_reward += reward\n",
    "        if isDone:\n",
    "            print('done!')\n",
    "            return total_reward\n",
    "        l_next, a_next = evaluate(team, env.current_state, epsilon, q_table)\n",
    "        if l_t.id != l_next.id:\n",
    "            update(q_table, team, l_next, a_t, l_t, reward)\n",
    "        a_t = a_next\n",
    "        l_t = l_next\n",
    "        t = t + 1\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run only to update local branch of tpg\n",
    "# current local branch [June 4 2021]: new-tpg \n",
    "# pip install ../PyTPG/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpg imports\n",
    "# import to do training\n",
    "from tpg.trainer import Trainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.agent import Agent\n",
    "# visual tools\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# for writing\n",
    "import csv\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 40.46000000000052, 0, 0, 0, 0, 0, 0, 41.01000000000041, 0.04, 0, 41.38000000000034, 0.04, 0, 0, 0, 0, 0, 40.640000000000484, 0, 0, 39.15000000000078, 0.04, 0, 0.04, 0, 0, 39.86000000000064, 0, 0, 0, 0, 40.5600000000005, 0.04, 0, 0, 0.04, 0, 0, 0, 0, 0, 0, 39.67000000000068, 0, 0.04, 40.840000000000444, 0, 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-7acbd04c0b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# evolution :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoreList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mteams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyScores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoreList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscoreStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitnessStats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tpg/trainer.py\u001b[0m in \u001b[0;36mapplyScores\u001b[0;34m(self, scores)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrootTeams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                         \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcomes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(actions=4, teamPopSize=50, pActAtom=1.0, \n",
    "                      nRegisters=4, initMaxActProgSize=48, gap=0.5)\n",
    "table = q_table()\n",
    "\n",
    "envName = 'GridWorld'\n",
    "env = GridWorld(5, 5, (0, 4), (4, 0))\n",
    "\n",
    "scoreList = []\n",
    "\n",
    "for gen in range(500):\n",
    "    agents = trainer.getAgents()\n",
    "    \n",
    "    if gen == 0:\n",
    "        table.create(agents)\n",
    "    \n",
    "    for agent in agents:\n",
    "        team = agent.team\n",
    "        #for team in agent.teams:\n",
    "        env.reset()\n",
    "        fitness = evaluate_fitness(table, team, env)\n",
    "        scoreList.append(fitness)\n",
    "        \n",
    "    # evolution :)\n",
    "    print(scoreList)\n",
    "    teams = trainer.applyScores(scoreList)\n",
    "    trainer.evolve(envName=[envName])\n",
    "    scoreStats = trainer.fitnessStats\n",
    "    \n",
    "    # evolve q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(actions=4, teamPopSize=50, pActAtom=1.0, \n",
    "                      nRegisters=4, initMaxActProgSize=48, gap=0.5)\n",
    "\n",
    "agents = trainer.getAgents()\n",
    "\n",
    "env = GridWorld(5, 5, (4, 0), (0, 4))\n",
    "\n",
    "action = env.sample_action()\n",
    "state, reward, isDone = env.step(action)\n",
    "\n",
    "for agent in agents:\n",
    "    team = agent.team\n",
    "#     for team in agent.team:\n",
    "    for learner in team.learners:\n",
    "        temp = team.id\n",
    "        print(temp)\n",
    "        print(learner.id)\n",
    "        print(str(learner.actionObj.actionCode))\n",
    "        print(learner.getAction((1, 1), False))\n",
    "        #actVars = None\n",
    "        #print(learner.bid((1, 1), actVars = actVars))\n",
    "        print('-----------')\n",
    "    print('HEYHEYHEY STARTSTARTSTART')\n",
    "    valid_learners = [lrnr for lrnr in team.learners if lrnr.isActionAtomic()]\n",
    "    print('VALID LERNERS')\n",
    "    print(valid_learners)\n",
    "    actVars = {'frameNum':-1}\n",
    "    print(actVars['frameNum'])\n",
    "    top_learner = max(valid_learners, key=lambda lrnr: lrnr.bid(state, actVars=actVars))\n",
    "    print(top_learner)\n",
    "    print('HEYHEYHEY   ENDENDENZD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
