{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qtpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downing fig 11 GridWorld\n",
    "#### GridWorld is based on fig 11 from Downing's \"Reinforced Genetic Programming\" paper\n",
    "![DowningFig11](downing_fig_11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 11 for testing on this\n",
    "class Figure11:\n",
    "    def __init__ (self, rows, cols, win_state, start_state):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = 20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def sample_action (self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if (rand >= 0) and (rand < 0.25):\n",
    "            return 0\n",
    "        elif (rand >= 0.25) and (rand < 0.5):\n",
    "            return 1\n",
    "        elif (rand >= 0.5) and (rand < 0.75):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        \n",
    "    def reset (self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (1, 2)) or (next == (1, 3)) or (next == (2, 2)) or (next == (2, 3)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += 0.1\n",
    "                #reward -= 0.01\n",
    "            else:\n",
    "                reward -= 0.01\n",
    "                #reward -= 1\n",
    "                #reward = reward\n",
    "        else:\n",
    "            reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += 100\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate\n",
    "    \n",
    "    def animate_path(self, sequence):\n",
    "        current_map = np.zeros((5, 5))\n",
    "        # add barrier\n",
    "        current_map[(1, 2)] = 5\n",
    "        current_map[(1, 3)] = 5\n",
    "        current_map[(2, 2)] = 5\n",
    "        current_map[(2, 3)] = 5\n",
    "        current_map[self.win_state] = 8\n",
    "\n",
    "        # animate the run!\n",
    "        for i in range(len(sequence)):\n",
    "            time.sleep(0.5)\n",
    "            if i == 0:\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(0)\n",
    "                print(current_map)\n",
    "            else:\n",
    "                current_map[sequence[i-1]] = 0\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(i)\n",
    "                print(current_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downing fig 12 GridWorld\n",
    "#### GridWorld is based on fig 12 from Downing's \"Reinforced Genetic Programming\" paper\n",
    "![DowningFig11](downing_fig_12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 12 for testing on this\n",
    "class Figure12:\n",
    "    def __init__ (self, rows, cols, win_state, start_state):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = 20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def sample_action (self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if (rand >= 0) and (rand < 0.25):\n",
    "            return 0\n",
    "        elif (rand >= 0.25) and (rand < 0.5):\n",
    "            return 1\n",
    "        elif (rand >= 0.5) and (rand < 0.75):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        \n",
    "    def reset (self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (2, 0)) or (next == (1, 1)) or (next == (2, 1)) or (next == (1, 3)) or (next == (2, 3)) or (next == (2, 4)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += 0.1\n",
    "                #reward -= 0.01\n",
    "            else:\n",
    "                reward -= 0.01\n",
    "                #reward -= 1\n",
    "                #reward = reward\n",
    "        else:\n",
    "            reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += 100\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate\n",
    "    \n",
    "    def animate_path(self, sequence):\n",
    "        current_map = np.zeros((5, 5))\n",
    "        # add barrier\n",
    "        current_map[(2, 0)] = 5\n",
    "        current_map[(1, 1)] = 5\n",
    "        current_map[(2, 1)] = 5\n",
    "        current_map[(1, 3)] = 5\n",
    "        current_map[(2, 3)] = 5\n",
    "        current_map[(2, 4)] = 5\n",
    "        current_map[self.win_state] = 8\n",
    "\n",
    "        # animate the run!\n",
    "        for i in range(len(sequence)):\n",
    "            time.sleep(0.5)\n",
    "            if i == 0:\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(0)\n",
    "                print(current_map)\n",
    "            else:\n",
    "                current_map[sequence[i-1]] = 0\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(i)\n",
    "                print(current_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downing fig 13 GridWorld\n",
    "#### GridWorld is based on fig 13 from Downing's \"Reinforced Genetic Programming\" paper\n",
    "![DowningFig11](downing_fig_13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 13 for testing on this\n",
    "class Figure13:\n",
    "    def __init__ (self, rows, cols, win_state, start_state):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = 20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def sample_action (self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if (rand >= 0) and (rand < 0.25):\n",
    "            return 0\n",
    "        elif (rand >= 0.25) and (rand < 0.5):\n",
    "            return 1\n",
    "        elif (rand >= 0.5) and (rand < 0.75):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        \n",
    "    def reset (self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (2, 0)) or (next == (1, 1)) or (next == (2, 1)) or (next == (1, 3)) or (next == (2, 3)) or (next == (3, 3)) or (next == (3, 4)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += 0.1\n",
    "                #reward -= 0.01\n",
    "            else:\n",
    "                reward -= 0.01\n",
    "                #reward -= 1\n",
    "                #reward = reward\n",
    "        else:\n",
    "            reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            reward -= 0.01\n",
    "            #reward -= 1\n",
    "            #reward = reward\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += 100\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate\n",
    "    \n",
    "    def animate_path(self, sequence):\n",
    "        current_map = np.zeros((5, 5))\n",
    "        # add barrier\n",
    "        current_map[(2, 0)] = 5\n",
    "        current_map[(1, 1)] = 5\n",
    "        current_map[(2, 1)] = 5\n",
    "        current_map[(1, 3)] = 5\n",
    "        current_map[(2, 3)] = 5\n",
    "        current_map[(3, 3)] = 5\n",
    "        current_map[(3, 4)] = 5\n",
    "        current_map[self.win_state] = 8\n",
    "\n",
    "        # animate the run!\n",
    "        for i in range(len(sequence)):\n",
    "            time.sleep(0.5)\n",
    "            if i == 0:\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(0)\n",
    "                print(current_map)\n",
    "            else:\n",
    "                current_map[sequence[i-1]] = 0\n",
    "                current_map[sequence[i]] = 1\n",
    "                clear_output(wait=True)\n",
    "                print(i)\n",
    "                print(current_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_table: \n",
    "    def __init__ (self):\n",
    "        self.q = []\n",
    "    \n",
    "    def create (self, agents):\n",
    "        for agent in agents:\n",
    "            team = agent.team\n",
    "            for learner in team.learners:\n",
    "                (self.q).append({'team': str(team.id), 'learner': str(learner.id), \n",
    "                                 'action': learner.actionObj.actionCode, 'q': 0})\n",
    "    \n",
    "    # add new learners upon evolution\n",
    "    def evolve (self, agents):\n",
    "        for agent in agents:\n",
    "            team = agent.team\n",
    "            # add new learners\n",
    "            for learner in team.learners:\n",
    "                found = 0\n",
    "                for entry in self.q:\n",
    "                    if (str(team.id) == entry['team']) and (str(learner.id) == entry['learner']) and (learner.actionObj.actionCode == entry['action']):\n",
    "                        found = 1\n",
    "                if found == 0:\n",
    "                    (self.q).append({'team': str(team.id), 'learner': str(learner.id), 'action': learner.actionObj.actionCode, 'q': 0})\n",
    "                        \n",
    "    # remove old learners after evolution\n",
    "    def clean (self, agents):\n",
    "        for agent in agents:\n",
    "            team = agent.team\n",
    "            # if entry is not in the team, remove entry from q table\n",
    "            for i in range(len(self.q)):\n",
    "                if (self.q[i])['team'] == str(team.id):\n",
    "                    found = 0\n",
    "                    for learner in team.learners:\n",
    "                        if  ((self.q[i])['learner'] == str(learner.id)) and ((self.q[i])['action'] == learner.actionObj.actionCode):\n",
    "                            found = 1\n",
    "                    if found == 0:\n",
    "                        print('removing: ' + (self.q[i])['team'])\n",
    "                        (self.q).pop(i)\n",
    "                \n",
    "    def update (self, team_id, learner_id, action, q_value):\n",
    "        (self.q).append({'team': str(team_id), 'learner': str(learner_id), 'action': action, 'q': q_value})\n",
    "    \n",
    "    def display (self):\n",
    "        for entry in self.q:\n",
    "            print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learners (team):\n",
    "    print('Getting learners for team: ' + str(team.id))\n",
    "    return team.learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (team, state, q_table, epsilon):\n",
    "    #learners = get_learners(team)\n",
    "    #top_bid = 0\n",
    "    top_learner = None\n",
    "    action = None   \n",
    "\n",
    "    # get best learner\n",
    "    actVars = {'frameNum':random.randrange(0, 100000000)}\n",
    "\n",
    "    valid_learners = [lrnr for lrnr in team.learners if lrnr.isActionAtomic()]\n",
    "    top_learner = max(valid_learners, key=lambda lrnr: lrnr.bid(state, actVars=actVars))\n",
    "\n",
    "#     for learner in learners:\n",
    "#         bid = learner.bid(state)\n",
    "#         if (bid > top_bid):\n",
    "#             top_bid = bid\n",
    "#             top_learner = learner \n",
    "\n",
    "    if top_learner == None:\n",
    "        print('No top learner found!')\n",
    "        return None, 0\n",
    "    else:\n",
    "        # e greedy action selection\n",
    "        e_prob = random.uniform(0, 1)\n",
    "\n",
    "        actions = []\n",
    "        top_q = 0\n",
    "        top_action = None\n",
    "        for entry in q_table.q:\n",
    "            if (entry['team'] == str(team.id)) and (entry['learner'] == str(top_learner.id)):\n",
    "                actions.append(entry['action'])\n",
    "                if entry['q'] > top_q:\n",
    "                    top_q = entry['q']\n",
    "                    top_action = entry['action']\n",
    "        \n",
    "        #print('Actions: ' + str(len(actions)))\n",
    "\n",
    "        if e_prob < epsilon:\n",
    "            if len(actions) == 1:\n",
    "                action = actions[0]\n",
    "            else:\n",
    "                rand_action = random.randint(0, len(actions)-1)\n",
    "                action = actions[rand_action]\n",
    "        else:\n",
    "            # select action with highest q value from top learner's actions\n",
    "            action = top_action\n",
    "    \n",
    "    return top_learner, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update (q_table, team, next_learner, action, learner, reward, alpha, discount):\n",
    "#     alpha = 0.1\n",
    "#     discount = 0.9\n",
    "    \n",
    "    # find the greatest q value out of possible actions for learner t+1\n",
    "    second_max_q = 0\n",
    "    for second_learner in q_table.q:\n",
    "        if second_learner['team'] == str(team.id) and second_learner['learner'] == str(next_learner.id):\n",
    "            if second_learner['q'] > second_max_q:\n",
    "                second_max_q = second_learner['q']\n",
    "    \n",
    "    # find the current learner and q update\n",
    "    for first_learner in q_table.q:\n",
    "        if first_learner['team'] == str(team.id) and first_learner['learner'] == str(learner.id) and first_learner['action'] == action:\n",
    "            # equation 1 from tpg pdf\n",
    "            first_learner['q'] += alpha * (reward + (discount * second_max_q) - first_learner['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fitness (q_table, team, env, epsilon, alpha, discount):\n",
    "    l_t, a_t = evaluate(team, env.current_state, q_table, epsilon)\n",
    "    t = 0\n",
    "    t_max = 50\n",
    "    total_reward = 0\n",
    "    while t < t_max:\n",
    "        s_next, reward, isDone = env.step(a_t)\n",
    "        total_reward += reward\n",
    "        if isDone:\n",
    "            return total_reward\n",
    "        l_next, a_next = evaluate(team, env.current_state, q_table, epsilon)\n",
    "        if l_t.id != l_next.id:\n",
    "            update(q_table, team, l_next, a_t, l_t, reward, alpha, discount)\n",
    "        a_t = a_next\n",
    "        l_t = l_next\n",
    "        t = t + 1\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run only to update local branch of tpg\n",
    "# current local branch [June 4 2021]: new-tpg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ../PyTPG/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpg imports\n",
    "# import to do training\n",
    "from tpg.trainer import Trainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.agent import Agent\n",
    "# visual tools\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# for writing\n",
    "import csv\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(actions=4, teamPopSize=50, pActAtom=1.0, \n",
    "                  nRegisters=4, initMaxActProgSize=48, \n",
    "                  initMaxTeamSize=2, maxTeamSize=5, gap=0.5) \n",
    "table = q_table()\n",
    "\n",
    "# environment select... should probably improve this someday...\n",
    "# envName = 'fig11'\n",
    "# env = Figure11(5, 5, (0, 4), (4, 0))\n",
    "envName = 'fig12'\n",
    "env = Figure12(5, 5, (0, 4), (4, 0))\n",
    "# envName = 'fig13'\n",
    "# env = Figure13(5, 5, (2, 4), (4, 0))\n",
    "\n",
    "allScores = []\n",
    "num_gen = 50\n",
    "champion = None\n",
    "best_score = -10000000\n",
    "\n",
    "\n",
    "# parameters\n",
    "epsilon = 0.5\n",
    "alpha = 0.1\n",
    "discount = 0.9\n",
    "\n",
    "for gen in range(num_gen):\n",
    "    scoreList = []\n",
    "    print('gen' + str(gen))\n",
    "    agents = trainer.getAgents()\n",
    "    \n",
    "    # update q table with new populations\n",
    "    if gen == 0:\n",
    "        table.create(agents)\n",
    "    else:\n",
    "        table.evolve(agents)\n",
    "        #table.clean(agents)\n",
    "    \n",
    "    for agent in agents:\n",
    "        team = agent.team\n",
    "        #for team in agent.teams:\n",
    "        env.reset()\n",
    "        fitness = evaluate_fitness(table, team, env, epsilon, alpha, discount)\n",
    "        \n",
    "        # save champion on last gen\n",
    "        if gen == (num_gen - 1):\n",
    "            if fitness > best_score:\n",
    "                best_score = fitness\n",
    "                print('Champ fitness: ' + str(fitness))\n",
    "                champion = team\n",
    "        \n",
    "        # apply scores\n",
    "        agent.reward(fitness, envName)\n",
    "        scoreList.append((agent.team.id, agent.team.outcomes))\n",
    "            \n",
    "    # evolution :)\n",
    "    teams = trainer.applyScores(scoreList)\n",
    "    trainer.evolve(tasks=[envName])\n",
    "    \n",
    "    # scores!\n",
    "    scoreStats = trainer.fitnessStats\n",
    "    allScores.append((scoreStats['min'], scoreStats['max'], scoreStats['average']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(num_gen):\n",
    "    x.append(i)\n",
    "\n",
    "for score in allScores:\n",
    "    y.append(score[2])\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Average Score')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all q values that correspond to given team\n",
    "def find_team_q (q_table, team):\n",
    "    result = []  \n",
    "    for entry in q_table.q:\n",
    "        if entry['team'] == str(team.id):\n",
    "            result.append(entry)\n",
    "    return result\n",
    "\n",
    "# TODO better organize this for quicker analysis\n",
    "def display_q (result):\n",
    "    for entry in result:\n",
    "        print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a given team after training\n",
    "def post_training_run (q_table, team, epsilon, alpha, discount):\n",
    "    env.reset()\n",
    "    epsilon = 0.5 # where should I define this??\n",
    "    l_t, a_t = evaluate(team, env.current_state, q_table, epsilon)\n",
    "    states = []\n",
    "    print(states)\n",
    "    states.append(env.current_state)    \n",
    "    t = 0\n",
    "    t_max = 50\n",
    "    total_reward = 0\n",
    "    while t < t_max:\n",
    "        print(states)\n",
    "        s_next, reward, isDone = env.step(a_t)\n",
    "        states.append(s_next)\n",
    "        #print(reward)\n",
    "        total_reward += reward\n",
    "        if isDone:\n",
    "            return states, total_reward\n",
    "        l_next, a_next = evaluate(team, env.current_state, q_table, epsilon)\n",
    "        if l_t.id != l_next.id:\n",
    "            print('Switched learners at ' + str(env.current_state))\n",
    "            update(q_table, team, l_next, a_t, l_t, reward, alpha, epsilon)\n",
    "        a_t = a_next\n",
    "        l_t = l_next\n",
    "        t = t + 1\n",
    "    return states, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tests on champion\n",
    "champ_table = find_team_q(table, champion)\n",
    "display_q(champ_table)\n",
    "for learner in champion.learners:\n",
    "    print(learner.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses same epsilon, alpha, and discount values as defined prior\n",
    "states, score = post_training_run(table, champion, epsilon, alpha, discount)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.animate_path(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
