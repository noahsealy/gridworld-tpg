{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qtpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downing fig 11 GridWorld "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridWorld is based on fig 11 from Downing's \"Reinforced Genetic Programming\" paper\n",
    "#### As shown below, it is a 5x5 maze\n",
    "![DowningFig11](downing_fig_11.png)\n",
    "#### The current implementation involves the following rewards per action:\n",
    "#####  +100 to score upon reaching the goal\n",
    "#####  -0.01 to score per action\n",
    "#####  -1 to score upon repeating a state that was repeated n amount of times (n currently is 20)\n",
    "#####  -1 to score upon hitting a wall or going out of bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use Downing fig 11 for testing on this\n",
    "class GridWorld:\n",
    "    def __init__ (self, rows, cols, win_state, start_state):\n",
    "        self.memory = []\n",
    "        self.memory_position = 0\n",
    "        self.memory_limit = 20\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start_state = start_state\n",
    "        self.win_state = win_state\n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def sample_action (self):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if (rand >= 0) and (rand < 0.25):\n",
    "            return 0\n",
    "        elif (rand >= 0.25) and (rand < 0.5):\n",
    "            return 1\n",
    "        elif (rand >= 0.5) and (rand < 0.75):\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "        \n",
    "    def reset (self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "        \n",
    "    # just reset for now...\n",
    "    def close (self):\n",
    "        self.current_state = self.start_state\n",
    "        return 1\n",
    "    \n",
    "    def check_win (self):\n",
    "        if self.current_state == self.win_state:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def step (self, action):\n",
    "        # north\n",
    "        if action == 0:\n",
    "            next = (self.current_state[0] - 1, self.current_state[1])\n",
    "        # south\n",
    "        elif action == 1:\n",
    "            next = (self.current_state[0] + 1, self.current_state[1])\n",
    "        # east\n",
    "        elif action == 2:\n",
    "            next = (self.current_state[0], self.current_state[1] + 1)\n",
    "        # west\n",
    "        else:\n",
    "            next = (self.current_state[0], self.current_state[1] - 1)\n",
    "\n",
    "        terminate = False\n",
    "        reward = 0\n",
    "        # check if move is legal\n",
    "        if (next[0] >= 0 and next[0] <= (self.rows-1)) and (next[1] >= 0 and next[1] <= (self.cols-1)):            \n",
    "            illegal = 0\n",
    "            if (next == (1, 2)) or (next == (1, 3)) or (next == (2, 2)) or (next == (2, 3)):\n",
    "                illegal = 1\n",
    "                    \n",
    "            if (illegal == 0):\n",
    "                self.current_state = next\n",
    "                reward += 0.01\n",
    "            else:\n",
    "                #reward -= 1\n",
    "                reward += 0\n",
    "        else:\n",
    "            #reward -= 1\n",
    "            reward += 0\n",
    "            \n",
    "        # punish repeat states within last 20 states\n",
    "        if self.current_state in self.memory:\n",
    "            #reward -= 1\n",
    "            reward += 0\n",
    "        \n",
    "        if self.check_win():\n",
    "            reward += 100\n",
    "            terminate = True\n",
    "        \n",
    "        # add new state to memory\n",
    "        if len(self.memory) <= self.memory_limit:\n",
    "            (self.memory).append(self.current_state)\n",
    "        # after memory is full, begin overriding it\n",
    "        else:\n",
    "            if self.memory_position < self.memory_limit:\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "                self.memory_position += 1\n",
    "            else:\n",
    "                self.memory_position = 0\n",
    "                self.memory[self.memory_position] = self.current_state\n",
    "        \n",
    "        return self.current_state, reward, terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_table: \n",
    "    def __init__ (self):\n",
    "        self.q = []\n",
    "    \n",
    "    def create (self, agents):\n",
    "        for agent in agents:\n",
    "            team = agent.team\n",
    "            for learner in team.learners:\n",
    "                (self.q).append({'team': str(team.id), 'learner': str(learner.id), 'action': learner.actionObj.actionCode, 'q': 0})\n",
    "    \n",
    "    def update (self, team_id, learner_id, action, q_value):\n",
    "        (self.q).append({'team': str(team_id), 'learner': str(learner_id), 'action': action, 'q': q_value})\n",
    "    \n",
    "    def display (self):\n",
    "        for entry in self.q:\n",
    "            print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learners (team):\n",
    "    print('Getting learners for team: ' + str(team.id))\n",
    "    return team.learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (team, state, epsilon, q_table):\n",
    "    #learners = get_learners(team)\n",
    "    #top_bid = 0\n",
    "    top_learner = None\n",
    "    action = None   \n",
    "\n",
    "    # get best learner\n",
    "    actVars = {'frameNum':random.randrange(0, 100000000)}\n",
    "    valid_learners = [lrnr for lrnr in team.learners if lrnr.isActionAtomic()]\n",
    "    top_learner = max(valid_learners, key=lambda lrnr: lrnr.bid(state, actVars=actVars))\n",
    "\n",
    "#     for learner in learners:\n",
    "#         bid = learner.bid(state)\n",
    "#         if (bid > top_bid):\n",
    "#             top_bid = bid\n",
    "#             top_learner = learner \n",
    "\n",
    "    if top_learner == None:\n",
    "        print('No top learner found!')\n",
    "        return None, 0\n",
    "    else:\n",
    "        # e greedy action selection\n",
    "        e_prob = random.uniform(0, 1)\n",
    "\n",
    "        actions = []\n",
    "        top_q = 0\n",
    "        top_action = None\n",
    "        for entry in q_table.q:\n",
    "#             print('Entry team id: ' + entry['team'])\n",
    "#             print('Team id: ' + team.id)\n",
    "#             print('Entry learner id: ' + entry['learner'])\n",
    "#             print('Top learner id: ' + top_learner.id)\n",
    "#             print(' ')\n",
    "#             print(' ')\n",
    "            if (entry['team'] == str(team.id)) and (entry['learner'] == str(top_learner.id)):\n",
    "                actions.append(entry['action'])\n",
    "                #print('Action: ' + str(entry['action']))\n",
    "                if entry['q'] > top_q:\n",
    "                    top_q = entry['q']\n",
    "                    top_action = entry['action']\n",
    "        \n",
    "        #print('Actions: ' + str(len(actions)))\n",
    "\n",
    "        if e_prob < epsilon:\n",
    "            if len(actions) == 1:\n",
    "                action = actions[0]\n",
    "            else:\n",
    "                rand_action = random.randint(0, len(actions)-1)\n",
    "                action = actions[rand_action]\n",
    "        else:\n",
    "            # select action with highest q value from top learner's actions\n",
    "            action = top_action\n",
    "    \n",
    "    return top_learner, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update (q_table, team, next_learner, action, learner, reward):\n",
    "    alpha = 0.1\n",
    "    discount = 0.1\n",
    "    \n",
    "    # find the greatest q value out of possible actions for learner t+1\n",
    "    second_max_q = 0\n",
    "    for second_learner in q_table.q:\n",
    "        if second_learner['team'] == team.id and second_learner['learner'] == next_learner.id:\n",
    "            if second_learner['q'] > second_max_q:\n",
    "                second_max_q = second_learner['q']\n",
    "    \n",
    "    # find the current learner and q update\n",
    "    for first_learner in q_table.q:\n",
    "        if first_learner['team'] == team.id and first_learner['learner'] == learner.id and first_learner['action'] == action:\n",
    "            # equation 1 from tpg pdf\n",
    "            first_learner['q'] += alpha * (reward + (discount * second_max_q) - first_learner['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fitness (q_table, team, env):\n",
    "    epsilon = 0.2 # where should I define this??\n",
    "    l_t, a_t = evaluate(team, env.current_state, epsilon, q_table)\n",
    "    t = 0\n",
    "    t_max = 1000\n",
    "    total_reward = 0\n",
    "    while t < t_max:\n",
    "        s_next, reward, isDone = env.step(a_t)\n",
    "        #print(reward)\n",
    "        total_reward += reward\n",
    "        if isDone:\n",
    "            print('done!')\n",
    "            return total_reward\n",
    "        l_next, a_next = evaluate(team, env.current_state, epsilon, q_table)\n",
    "        if l_t.id != l_next.id:\n",
    "            update(q_table, team, l_next, a_t, l_t, reward)\n",
    "        a_t = a_next\n",
    "        l_t = l_next\n",
    "        t = t + 1\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run only to update local branch of tpg\n",
    "# current local branch [June 4 2021]: new-tpg \n",
    "# pip install ../PyTPG/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tpg imports\n",
    "# import to do training\n",
    "from tpg.trainer import Trainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.agent import Agent\n",
    "# visual tools\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# for writing\n",
    "import csv\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen0\n",
      "[(UUID('36d3f5d8-8b2a-49c1-8aa9-328e16d08a94'), {'GridWorld': 0.04}), (UUID('c2dd8a66-9633-4035-8509-ded65f2c5c1c'), {'GridWorld': 0}), (UUID('0e2e23ae-61db-4d5a-937c-a2379924dced'), {'GridWorld': 0.04}), (UUID('9949e25a-9475-40f7-9431-67f3651f8ba5'), {'GridWorld': 0}), (UUID('89209706-e7f3-4e36-a59f-6a17d1d84ba4'), {'GridWorld': 0}), (UUID('0e1e1cae-362f-4f47-a643-0359d3062813'), {'GridWorld': 3.649999999999966}), (UUID('bab72dfe-2045-4d27-b22d-ef932ebef5c3'), {'GridWorld': 3.93999999999996}), (UUID('42c76b12-0fc8-44b0-8d88-66436f513a11'), {'GridWorld': 3.8199999999999625}), (UUID('8cbcdddd-dc23-41c1-8ff2-45b402942a97'), {'GridWorld': 0}), (UUID('5c2e5a0f-d042-4518-bf3c-d494c59f8e28'), {'GridWorld': 0}), (UUID('4ce0dda2-77c1-4faa-9bb0-168f776998a8'), {'GridWorld': 0.04}), (UUID('3ab78242-439f-49f1-944b-62c09466007c'), {'GridWorld': 0.04}), (UUID('491affce-a230-4e93-8cd8-ef0c3cdd6310'), {'GridWorld': 2.5499999999999896}), (UUID('8dae5651-57e7-4440-a865-f5ed6e3b4df8'), {'GridWorld': 3.319999999999973}), (UUID('c5b61ef6-a8d4-4c3f-b355-0d9090be7aed'), {'GridWorld': 3.659999999999966}), (UUID('090423e2-dcbd-4587-8bbc-d5139c256786'), {'GridWorld': 0}), (UUID('00556e87-ff0e-42e7-8f09-16263f4adec9'), {'GridWorld': 0}), (UUID('099013c5-86a1-4151-865f-c3235e4523f0'), {'GridWorld': 0}), (UUID('5b2a9d90-678f-430c-84e4-48adab8b30d5'), {'GridWorld': 0}), (UUID('ffea9683-98e1-4e09-bcc1-18ce5e897d7f'), {'GridWorld': 0.04}), (UUID('a47b7491-5773-4623-821a-4aa47907cb78'), {'GridWorld': 0.060000000000000005}), (UUID('d208e16b-f346-4386-9fb5-67c7979d7dd5'), {'GridWorld': 0.04}), (UUID('cc188d46-fa76-4db7-acc8-8fe5d2e2c3eb'), {'GridWorld': 0}), (UUID('43fbaca1-55de-4d1c-91a4-7becc09500e3'), {'GridWorld': 4.179999999999955}), (UUID('2636e166-b41e-4bbf-bc46-97e96d5c41aa'), {'GridWorld': 0}), (UUID('d4e00bb5-80fd-40da-a746-31e36f7435e5'), {'GridWorld': 0}), (UUID('b925611d-27ba-4013-8c8e-812cf1ae145d'), {'GridWorld': 4.039999999999958}), (UUID('4bce5141-d580-45ad-9f5a-81f55c7d1051'), {'GridWorld': 0.04}), (UUID('1c7da66e-6002-4400-bef0-f54372539f8c'), {'GridWorld': 0}), (UUID('7a0453a8-4d3e-4077-8261-684de6a93241'), {'GridWorld': 0.04}), (UUID('40784384-e820-4a4a-9062-c26f90c649b4'), {'GridWorld': 0.04}), (UUID('bf2c1893-abb8-42f8-83e6-771887835363'), {'GridWorld': 1.470000000000001}), (UUID('a998fc0d-b04e-40ec-9e8e-8aa74baec1bd'), {'GridWorld': 0.04}), (UUID('b2a0d700-560a-4dfe-9340-d7c52d9fa51f'), {'GridWorld': 0}), (UUID('158fd702-e710-40cb-8bfd-5b72294f004a'), {'GridWorld': 0.04}), (UUID('91199042-353b-45ef-8112-08b9d0d3d3a3'), {'GridWorld': 0}), (UUID('91d079b4-245b-43fe-a613-b42cdbaae588'), {'GridWorld': 0}), (UUID('98b6f47b-e6d9-4a5e-bbc8-40ac3c5a9f97'), {'GridWorld': 0.04}), (UUID('5aaf74ee-a801-4e4c-bcdf-29cba607fb58'), {'GridWorld': 0}), (UUID('c6e41500-19a6-4ae9-b776-da7364279b29'), {'GridWorld': 0.04}), (UUID('9c7681b5-b5b4-4349-a1d0-444c4f34a3c7'), {'GridWorld': 2.0799999999999996}), (UUID('09b58cc2-fce8-41ca-9793-47b5a0ee97df'), {'GridWorld': 0.02}), (UUID('c6afd256-ef2d-40a0-be30-b91441890d93'), {'GridWorld': 3.7699999999999636}), (UUID('289b87c7-870b-4e5a-ba61-74517d98a7a9'), {'GridWorld': 0}), (UUID('db90bc64-79e2-4b3d-ad8e-d050ee28d349'), {'GridWorld': 0.04}), (UUID('1cb09ab9-4fec-489e-a265-0aeb9840eef2'), {'GridWorld': 0.04}), (UUID('7fa51dbe-365a-4479-8788-52c490ed78fd'), {'GridWorld': 0.04}), (UUID('1b4592e1-d053-400c-87b5-e4a1545243c4'), {'GridWorld': 0.04}), (UUID('64e7e2af-0e93-4b81-b0f4-eb22d9d075e2'), {'GridWorld': 1.0800000000000007}), (UUID('71a42188-d11a-4d65-9228-65eb5d3baa92'), {'GridWorld': 0})]\n",
      "gen1\n",
      "done!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-712a29783e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#for team in agent.teams:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# apply scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-307ff8a734e4>\u001b[0m in \u001b[0;36mevaluate_fitness\u001b[0;34m(q_table, team, env)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0ml_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ml_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-15539be10de7>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(team, state, epsilon, q_table)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mrand_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_randbelow_with_getrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty range for randrange() (%d, %d, %d)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Non-unit step argument supplied.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty range for randrange() (0, 0, 0)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(actions=4, teamPopSize=50, pActAtom=1.0, \n",
    "                      nRegisters=4, initMaxActProgSize=48, gap=0.5)\n",
    "table = q_table()\n",
    "\n",
    "envName = 'GridWorld'\n",
    "env = GridWorld(5, 5, (0, 4), (4, 0))\n",
    "\n",
    "scoreList = []\n",
    "\n",
    "for gen in range(500):\n",
    "    print('gen' + str(gen))\n",
    "    agents = trainer.getAgents()\n",
    "    \n",
    "    if gen == 0:\n",
    "        table.create(agents)\n",
    "    \n",
    "    for agent in agents:\n",
    "        team = agent.team\n",
    "        #for team in agent.teams:\n",
    "        env.reset()\n",
    "        fitness = evaluate_fitness(table, team, env)\n",
    "        \n",
    "        # apply scores\n",
    "        agent.reward(fitness, envName)\n",
    "        scoreList.append((agent.team.id, agent.team.outcomes))\n",
    "        \n",
    "        \n",
    "    # evolution :)\n",
    "    print(scoreList)\n",
    "    teams = trainer.applyScores(scoreList)\n",
    "    trainer.evolve(tasks=[envName])\n",
    "    scoreStats = trainer.fitnessStats\n",
    "    \n",
    "    # evolve q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
